\documentclass[a4paper,12pt]{report}

\usepackage{GStyle} % Compile only with XeTeX!

\renewcommand{\GTitle}{Конспект лекций по дисциплине}
\renewcommand{\GCaption}{"Теория информации"}
\renewcommand{\GGTitle}{\vspace{-\baselineskip}}
\renewcommand{\GGCaption}{\vspace{-\baselineskip}}

\begin{document}

\maketitle
\maketoc

\section{Теория вероятности}





\subsection{Основные сведения из теории вероятности}





%\wdate{05.09.13}
\subsubsection{Введение}



%\paragraph{Классическое определение вероятности}


	Термин \term{информация} в курсе будет пониматься в узком научном смысле.

	\term{Теория информации} – специальная математическая дисциплина. Её содержанием является абстрактно формулируемые теоремы и модели. ТИ имеет обширное применение к теории передачи сообщений, записывающих устройств, матлингвистике, компьютерной технике.

	В самом общем виде теория информации понимается как теория передачи сигналов по линиям связи. Наиболее важное понятие ТИ –- сама информация. В нашей жизни большую роль играет информация и связанные с ней операции: передача, получение, обработка, хранение.

	Информация имеет две стороны: количественную и качественную. Иногда важно получение общего количества информации (количественная сторона), иногда важно конкретное содержание самой ИИ. Отметим, что переработка ИИ является технически сложной процедурой, которая усложняет разработку общей теории информации.

	Важнейшим этапом в открытии основных закономерностей ТИ были работы американского инженера-связиста, математика Клода Шеннона (1947-49гг). 

	Для вычисления количества информации была предложена т.н. \term{логарифмическая мера}. Понятие \term{количества информации} тесно связано с понятием энтропии как меры степени неопределённости. Приобретение информации сопровождается уменьшением неопределённости, следовательно, количество информации можно измерять количеством ''исчезнувшей неопределённости'' (энтропии).

	Теория информации является математической теорией, использующей понятия и методы теории вероятности.





\subsubsection{Вероятность. Случайные события и величины}

	Пусть производится серия из $N$ опытов, причём некоторое событие $A$ происходит в $N_a < N+1$. Тогда $h_n(A) = N_a/N $ называется частотой появления события $A$ в серии из $N$ опытов. 
	Известный факт: с ростом $N \quad h_n(A) \rightarrow p$ (постоянная $p$ - вероятность появления случайного события $A$).

	Наука, изучающая свойства вероятности и применение этого понятия называется \term{теория вероятности}.

	Событие, которое при выполнении некоторого комплекса условий обязательно выполняется называется \term{достоверным событием}.

	Событие, которое при выполнении некоторого комплекса условий не выполняется называется \term{невозможным событием}.

	\sample{Пример:} Выпадение определённого числа очков на грани игральной кости  - достоверное событие.

	Выпадение семи очков на грани игральной кости - невозможное событие.

	\term{Случайное событие} – событие, которое может произойти, а может и не произойти.


	\task{Задача:} В урне 10 шаров : 5 белых, 3 чёрных и 2 красных. Найти вероятность выпадения шара определённого цвета (шары одинаковы).

	\rtask{Решение:} Выписать случайные события:


	\begin{tabular}{rll}

		$A$ & –- \{вынутый шар белый\} & $P(A) = 5/10 = 1/2$;\\
		$B$ & –- \{вынутый шар чёрный\} & $P(B) = 3/10$;\\
		$C$ & –- \{вынутый шар красный\} & $P(C) = 2/10 = 1/5$.\\

	\end{tabular}


	\task{Задача:} Какова вероятность, что при бросании кости выпадет число очков,~кратное~3?

	\rtask{Решение:}\strut


	\begin{tabular}{l}

	Кратны $3\,\{3,6\}$.\\ 

	$N$ исходов $ = 6$.\\

	$P(A) = 2/6 = 1/3$.\\

	\end{tabular}


	\sstrut Общий принцип решения задач сводится к понятию равновероятности или равновозможности. (например, все грани кости одинаковы, и вероятность выпадения той или иной грани равна $1/6$).



\subsubsection{Классическое определение вероятности}

	Пусть из $N$ возможных исходов опыта случайное событие $A$ появляется $M$ раз. Тогда вероятность случайного события $A$ в модели с равновероятными исходами вычисляется по формуле $P(A) = M/N$ 

	Каждому опыту отвечает своя таблица вероятности. К примеру, в задаче с урнами и шарами таблица вероятности имеет вид: \strut


	\begin{tabular}{|l|c|c|c|}
	\hline
		События & A & B & C\\
	\hline	
		Вероятности & $P(A) = 1/2$ & $P(B) = 3/10$ & $P(C) = 1/5$\\
	\hline
	\end{tabular}


	\strut Можно сказать, что в опыте с бросанием кости число очков, выпадающих на грани является случайной величиной, которая может принимать одно из возможных 6 числовых значений в зависимости от случая.

	Итак, случайная величина –- числовая функция, принимающая то или иное числовое значение в зависимости от случая.

	Например, количество рождений в городе за год - случайная величина.





\subsubsection{Свойства вероятности. Сложение и умножение случайных событий.}





\paragraph{Несовместные и независимые случайные события.}

	Из определения вероятности вытекают основные свойства вероятности случайного события $A$:


	\begin{itemize}
	
	\item	$0 \leqslant P(A) \leqslant 1$.

	$P(A) = 1$ – достоверное событие;

	$P(A) = 0$ – невозможное событие.


	\item	Пусть опыт приводит к двум взаимоисключающим событиям или исходам $A$ или $B$. В этом случае B называют противоположным $A$ событием $(B = \bar A)$

		Пусть $P(A) = \frac{m}{n}$;

		Тогда $P(\bar A) = \frac{(n-m)}{n} 
				 = 1 - \frac{m}{n} 
				 = 1 - P(A) \Rightarrow P(A) 
				 = 1 - P(\bar A)$.

	Пусть случайное событие $A_1 \subset A$ влечёт появление события $A \Rightarrow P(A1) < P(A)$


	\item	\stitle{Правило сложения вероятностей для двух событий:}


		\begin{itemize}
		
		\item	Пусть $A$ и $B$ – несовместны. 

			Тогда $A \cap B = \eset$;

			$P(A) = \frac{m_1}{n}$; 

			$P(B) = \frac{m_2}{n}$;

			$P(A+B) = \frac{(m_1+m_2)}{n} 
				= \frac{m_1}{n} + \frac{m_2}{n} 
				= P(A) + P(B)$.

			Таким образом, $P(A+B) = P(A) + P(B)$.

			\medskip

			В примере с урной вероятность извлечь чёрный или белый шар равна 

			$P(A+B) = P(A) + P(B) 
				= \frac{1}{2} + \frac{3}{10} 
				= \frac{4}{5}$;

			\comm{Замечание:} Пусть некоторый опыт проиводит к появлению $K$ различных (взаимоисключающих) исходов: \strut


			\begin{tabular}{|l|c|c|c|c|}
			\hline
				Исходы & A1 & A2 & \sudots & An\\
			\hline
				Вероятности & P1 & P2 & \sudots & Pn\\
			\hline
			\end{tabular}	


			\strut Заметим, что бывают случаи, когда 

			$$
			  \sum_{i=1}^{k}P(A_i) 
			       = P(A_1+A_2+ \sudots +A_n) = 1
			$$

			В этом случае говорят, что события 
			   $A_1,A_2, \sudots ,A_n$ 
			составляют \term{полную} \linebreak \term{группу} случайных событий, то есть 
			   $A_1,A_2, \sudots ,A_n$
			попарно несовместны.

			$A_1,A_2, \sudots ,A_n : A_i \cap A_j 
			   = \eset \, \forall \, i,j: i \not= j$;
			если $A_1+A_2+ \sudots +A_n$ - достоверное событие.


		\item	Пусть $A$ и $B$ совместны. 
			$P(A+B) = P(A)+P(B)-P(AB)$,
			где $P(AB)$ – вероятность одновременного происхождения двух случайных событий $A$ и $B$.

		\end{itemize}			
	
	
	\stitle{Теорема сложения вероятности для совместных случайных событий.}
			 
			 (диаграмма Венна: $A = m_1; B = m_2, A \cap B = l$).

			$P(AB) = \frac{l}{n}$

			$P(A+B) = \frac{(m_1+m_2-l)}{n} 
				= \mbox{ (в $m_1$ и $m_2$ входит $l$) } 
				= \frac{m_1}{n} + \frac{m_2}{n} - \frac{l}{n}$

			События $A$ и $B$ называются \term{независимыми}, если результат выполнения события $A$ не связан с результатом события $B$. (извлечение двух чёрных шаров из разных урн – независимые события)


	\item	\stitle{Теорема умножения вероятности для двух независимых событий:}

		Если $A$ и $B$ независимы, то $P(AB) = P(A)*P(B)$

	\end{itemize}


	\sample{Пример 1:}  Какова вероятность при двух бросках монеты оба раза выпадет орёл?


	\begin{tabular}{ll}
	
		$P(AB) = \,\, ?$ & \\
	
		$A\mbox{\{орёл\}}$ & $ P(A) = \frac{1}{2}$;\\
	
		$B\mbox{\{решка\}}$ & $ P(B) = \frac{1}{2}$;\\
	
		& $P(AB) = \frac{1}{4}$.\\

	\end{tabular}

	
	\sample{Пример 2:} В колоде 52 карты, 4 масти, 2 козыря. Какова вероятность того, что взятая наугад карта 2 является тузом или козырем?


	\begin{tabular}{ll}
	
		$A\{\mbox{туз}\} \qquad$ & $P(A) = 1/13$;\\
	
		$B\{\mbox{козырь}\} \qquad$ & $P(B) = 1/4$;\\
	
		$P(AB) = 1/52$; & \\

		\multicolumn{2}{l}{$A$ и $B$ совместны, независимы.}\\

		\multicolumn{2}{l}{$P(A+B) = P(A) + P(B) - P(AB) = 1/13 + 1/4 - 1/52 = 4/13$}.\\

	\end{tabular}





%\wdate{19.09.13}

\subsubsection{Условная вероятность.}

	\sample{Рассмотрим пример:} В урне $M$ чёрных шаров и $N-M$ белых. 
	Случайное событие 
	\\$A$ \{извлечение чёрного шара\} и 
	\\$B$ \{извлечение чёрного шара из той-же урны после того, как из неё уже вынут один шаp\}

	$$
	  P(B|A) = \frac{(m-1)}{(n-1)}
	$$ 

	Поскольку, если событие $A$ имело место, то в урне осталось $M-1$ чёрных шаров.

	$$
	  P(B|\bar A) = \frac{m}{(n-1)}
	$$

	$\bar A : \{$первый вынутый шар - белый$\}$

	Вероятность события $B$ здесь разная. Вероятность, которую имеет событие $B$ в том, случае, когда известно, что событие $A$ имело место называется \term{условной вероятностью} события $B$ при условии выполнения события $A$.	

	$$
	  P(B/A) = P(B|A) = P_A(B)
	$$

	Условные вероятности можно вычислять аналогично вычислению безусловных вероятностей. 

	В случае если $A$ и $B$ независимы, $P(A|B) = P(A)*P(B)$. 

	В случае зависимости $P(AB) = P(A)*P(B|A) = P(B)*P(A|B)$. 

	В обоих случаях мы имеем правило умножения вероятностей. В одном случае для независимых событий, в другом для зависимых. Последнее соотношение часто кладут в определение условной вероятности. 

	$$
	  P(B|A) = \frac{P(AB)}{P(A)} \qquad 
	  P(A|B) = \frac{P(AB)}{P(B)}
	$$

	Из предыдущей формулы можем составить пропорцию: 
	
	$$
	  \frac{P(B|A)}{P(B)} = \frac{P(A|B)}{P(A)}
	$$

	Из определения условной вероятности вытекают ее основные свойства:


	\begin{enumerate}

	\item	$0 \leqslant P(B|A) \leqslant 1$, причём $P(B|A) = 1$ когда $A \subset B$; $B$ - достоверное случайное событие.

		$P(B|A) = 0 \Longleftrightarrow A, B$ несовместны, или известно, что $B$ – невозможное событие.

	
	\item	Пусть $B_1 \subset B$ (появление $B_1$ вызывает событие $B$). \; $P(B1|A) \leqslant P(B|A)$.


	\item	Если $B$ и $C$ несовместны $P(B+C|A) = P(B|A) + P(C|A)$ (теорема сложения вероятностей для несовместных событий)


	\item	$P(\bar B|A) =  1 - P(B|A)$

	\end{enumerate}


	\comm{Замечание:} Пусть имеется $K$ (и только $K$) попарно несовместных исходов некоторого опыта $A_1,A_2, \sudots ,A_k$, называемых гипотезами. Пусть некоторое случайное событие $B$ может произойти при выполнении одной из гипотез. Тогда очевидно, что $B = A_1B + A_2B + \sudots + A_kB$ (все события $A_iB$ несовместны, поэтому можно воспользоваться теоремой сложения вероятностей)

	$$
	  P(B) = P\left(\sum^k_{i=1}A_iB\right)
	       = \sum^k_{i=1}P(A_iB) 
	       = \sum^k_{i=1}\left(P(A_i)*P(B|A_i)\strut\right)
	$$

	Формула носит название формулы полной вероятности 

	$$
	  P(B) = \sum^k_{i=1}P(A_i)*P(B|A_i)
	$$

	
	\task{Задача: } Имеется 5 урн : в двух по одному белому и пять чёрных шаров; в одной – 2 белых, 5 чёрных; в двух – 3 белых, 5 чёрных шаров. Наудачу выбирается одна урна. Из неё извлекается один шар. Какова вероятность того, что шар белый?

	\rtask{Решение:} Выберем в качестве гипотез 3 способа \strut


	\begin{tabular}{ll@{\qquad}l}
	
		$A_1$ : \{Выбрана урна с 1 б.ш\} &  $P(A_1) = 2/5$ & $P(B|A_1) = 1/6$ \\
	
		$A_2$ : \{Выбрана урна с 2 б.ш\} & $P(A_2) = 1/5$ & $P(B|A_2) = 2/7$ \\
	
		$A_3$ : \{Выбрана урна с 3 б.ш\} & $P(A_3) = 2/5$ & $P(B|A_3) = 3/8$ \\
	
		$B$ = {извлечён белый шар} \qquad & & \\
		$P(B) = \frac{1}{6} * \frac{2}{5} + \frac{2}{7} * \frac{1}{5} + \frac{3}{8} * \frac{2}{5} = \frac{23}{84}$
	
	\end{tabular}





\subsubsection{Математическое ожидание случайной величины и его основные свойства}



	\paragraph{Введение.}

	Важнейшей числовой характеристикой $\xi$ является её математическое ожидание или среднее значение, вычисляемое по правилу $M\xi = \sum\limits^n_{i=1}x_ip_i)$, где $x_i$ – принимаемые $\xi$ значения, $p_i$ – вероятности их выпадения. 

	С помощью математического ожидания мы можем сравнивать между собой две случайные величины (например, из двух стрелков лучший тот, кто выбивает в среднем наибольшее число очков), однако встречаются задачи, в которых знание одного лишь $M\xi$ недостаточно. 


	\sample{Пример:} Пушка ведёт прицельный огонь по мишени, удалённой от пушки на расстояние $a$. Обозначим дальность полёта снаряда через $\xi$ километров; $M\xi = a$

	Отклонение $M\xi$ от $a$ свидетельствует о наличии систематической ошибки (производственный дефект, неправильный угол наклона). Ликвидация систематической ошибки достигается изменением угла наклона орудия. 

	Вместе с тем, отсутствие систематической ошибки ещё не гарантирует высокую точность стрельбы. Чтобы оценить точность надо знать, насколько близко ложатся снаряды к цели. 

	Как определить точность стрельбы и сравнить между собой качество стрельбы двух орудий?

	Отклонение снаряда от цели - $\xi - a$
	
	$M(\xi - a) = M\xi - a = a - a = 0$

	В среднем, положительные и отрицательные значения $M\xi$ сокращаются. Поэтому принято характеризовать разброс значений случайной величины математическим ожиданием квадрата её отклонения от своего математического ожидания. Полученное таким образом число называется дисперсией случайной величины $\xi$. 

	$D\xi = M(\xi-a)^2 = M[\xi-M\xi]^2$

	Ясно, что в случае орудий, ведущих стрельбу, лучшим следует считать орудие, у которого $D\xi$ будет наименьшей.

	Пусть $\xi$ характеризуется таблицей вероятностей \strut


	\begin{tabular}{|r|c|c|c|c|}
	\hline
		$x_i:$ & $x_1$ & $x_2$ & $\ldots$ & $x_n$\\
	\hline	
		$p_i:$ & $p_1$ & $p_2$ & $\ldots$ & $p_n$\\
	\hline
	\end{tabular}

	
	$$
	  M\xi = \sum^n_{i=1}x_ip_i;
	  \qquad
	  D\xi = M (\xi - M\xi)^2 
	       = \sum^n_{i=1}(x_i-M\xi)^2*p_i
	$$



\subsubsection{Определение математического ожидания}

	Пусть есть некоторое пространство, в котором имеется некоторое $\xi = \xi(\omega_i)$.

	$\omega_i$ – неразделимое событие (пример: исходы броска монеты); $\omega_i:(i=1,\bar n)$.

	Совокупность $\omega_i$ образует пространство элементарных событий
	$\Omega = \{ \omega_1, \omega_2, \sudots, \omega_n \}$

	\term{Математическим ожиданием} случайной величины $\xi$ называется число, обозначаемое $M\xi$ и равное 

	$$
	  M\xi = \sum_{\omega_i \in \Omega}\{(\omega_i)*P(\omega_i)\} 
	       = \sum^n_{i=1}\xi(\omega_i)*p(\omega_i)
	       \mbox{, где $p_i$ - элементарные вероятности.}
	$$

	Из определения математического ожидания вытекают следующие свойства:


	\begin{enumerate}

	\item	Аддитивность. $M(\xi + \eta) = M\xi + M\eta$. 

		Следствие $M\left(\sum\limits^n_{k=1}\strut\xi_k\right)=\sum\limits^n_{k=1}(M\xi_k)$.


	\item	$\forall C = const: M(C*\xi) = C*M\xi$. Совокупность свойств 1 и 2 даёт нам свойство линейности математического ожидания:

	$$
	  M(C_1\xi_1 + C_2\xi_2 + \sudots + C_n\xi_n) 
	    = C_1M(\xi_1) + C_2M(\xi_2) + \sudots + C_nM(\xi_n)
	$$


	\item	Математическое ожидание индикатора случайного события равно вероятности этого случайного события. 

	Индикатор $[\chi]$: $M\chi_A (\omega) = P(A)$ - случайная величина, принимающая 2 значения: 
		$
		  \chi_A(\omega) = \{1, \omega \in A \,|\, 0, \omega \not\in A\}
		$

		$$
		  \sum_{\omega \in A}P(\omega) = P(A)
		$$

		$$
		  M\chi_A(\omega) = \sum_{\omega \in A}1*p(\omega) + \sum_{\omega \not\in A}0*p(\omega)
		                  = \sum_{\omega \in A}1*p(\omega) = P(A).
		$$

	
	\item	Свойство монотонности $\xi \geqslant \eta \Rightarrow M\xi \geqslant M\eta$.

	\end{enumerate}


	Докажем вначале, что имеет место следующее свойство $\xi \geqslant 0 \Rightarrow M\xi \geqslant 0$ (при разложении по определению неотрицательны).

	$$
	  M\xi = \sum_\omega\xi(\omega)p(\omega) \geqslant 0.
	$$
	
	Применим полученное свойство:

	$$
	  \xi - \eta \ge 0 \Rightarrow 
	   M(\xi - \eta) \ge 0 \Rightarrow 
	   M\xi - M\eta \ge 0 \Rightarrow 
	   M\xi \ge M\eta.
	$$



\subsubsection{Формулы вычисления математического ожидания}

	Пусть $x_1,x_2, \sudots ,x_n$ –-  значения случайной величины $\xi$, принимаемые с вероятностями $p_1, \sudots ,p_i$. Тогда имеет место следующая формула для вычисления математического ожидания :

	$$
	  M\xi=\sum^n_{i=1}x_i*P(\xi=x_i)
	$$

	Чтобы доказать формулу будем исходить из того, что $\xi$ может быть представлена в виде линейной комбинации индикаторов случайных событий

	$$
	  \xi = \sum^n_{i=1}x_i*\chi_{A_i}(\omega)
	$$ $$
	  A_i \{\omega_i : \xi = x_i\}
	$$

	Левые и правые части соотношения совпадают. Применим к написанному равенству операцию математического ожидания: 

	$$	
	  M\left(\sum^n_{i=1}x_i\chi_{A_i}(\omega)\right) 
	    = \sum^n_{i=1}M\left(x_i\strut\chi_{A_i}(\omega)\right) 
	    = \sum^n_{i=1}x_iM\left(\strut\chi_{A_i}(\omega)\right) 
	    = \sum^n_{i=1}x_iP(\xi=x_i)	
	$$

	Рассуждая аналогично, нетрудно получить формулы вычисления математического ожидания от величин, представляющих собой функции случайных величин. 

	Пусть заданы $f(\xi),g(\xi,\eta)$.

	В этом случае 

	$$
	  M(f(\xi)) = \sum^n_{i=1}\left(f(x_i)*P(\xi=x_i)\right) 
	$$

	$$
	  M(g(\xi,\eta)) = \sum^n_{i=1}
	    \left(   
	      \sum^m_{j=1}g(x_i,y_j)*P
	        \left(
	          \xi=x_i,\strut\,\eta=y_i
	        \right) \!
	    \right) 
	$$

	Здесь $P(\xi,\eta)$ – совместная вероятность.


	\begin{enumerate}

	\item[5] Мультипликативное свойство математического ожидания

		Пусть $\xi,\eta$ - независимые случайные величины, то $M(\xi,\eta) =  M\xi * M\eta$

		Доказательство:

		$$
		  M(\xi,\eta) = \sum^n_{i=1}
		    \left(
		      \sum^m_{j=1}x_i*y_j*\strut P
		        \left(
		          \xi=x_i,\strut\eta=y_j
		        \right) \!
		    \right)
		$$

		Если $\xi,\eta$ независимы, то для них применима теорема умножения вероятности. 

		$$
		  P(\xi=x_i,\eta=y_j) 
		    = (\xi,\eta \mbox{ независимы})
		    = P(\xi=x_i)*P(\eta=y_j)
		$$
		
		$$
		  \sum^n_{i=1}
		    \left(
		      \sum^m_{j=1}x_i*y_j*\strut P(\xi=x_i)*P(\eta=y_j)
		    \right)
		  =
		$$
		
		$$
		  = \sum^n_{i=1}x_i*P(\xi=x_i)  \, * \, 
		    \sum^m_{j=1}y_i*P(\eta=y_j) 
		  = M\xi*M\eta
		$$

	\end{enumerate}


	\comm{Замечание:} Все написанные формулы имеют место, если вероятностное пространство конечно, т.е. число элементарных событий конечно $\omega_i = (1,\bar n)$. 

	В случае, если вероятностное пространство счётно, количество элементарных сообщений бесконечно, тогда для случайной величины 
	$
	  \xi(\omega), 
	  \omega \in \mbox{(счетное вероятностное пространство)}
	$
	имеют место следующие формулы:

	$$
	  \omega_i, i = [\overline{1,\strut\infty}]
	$$

	$$
	  M\xi = \sum^\infty_{i=1}\left(x_i*P(\xi=x_i)\right)
	$$

	$$
	  Mf(\xi) = \sum^\infty_{i=1}\left(f(xi)*P(\xi=x_i)\right)
	$$

	В формулах справа стоят ряды. Чтобы математические ожидания существовали надо, чтобы эти ряды сходились. Ряд сходится, если он имеет конечную сумму.

	
	\task{Задача:} Вычислить $M\xi$, распределённой по закону Пуассона. $P(\xi=k)=(a^k/k!)e^{-a}$,
	  где $k=\{0,1,2,3,4, \sudots ,\infty\}; \; a>0$ – заданный заранее характер распределения. 

	\rtask{Решение:} 

	$$
	  M\xi = \sum^\infty_{k=0}k*\frac{(a^k*e^{-a})}{k!}
	       = e^{-a}\sum^\infty_{k=0}k*\frac{(ka^k)}{k!}
	       =
	$$ $$
	       = e^{-a}\sum^\infty_{k=0}\frac{(k*a^{k-1}a)}{(k-1)!}
	       = e^{-a}a\sum^\infty_{s=0}\frac{a^s}{s!}\mbox{\scriptsize\quad (формула Маклорена)}
	       = e^{-a}ae^a = a
	$$ 
	
	

	Математическое ожидание случайной величины,  распределённой по закону Пуассона с параметром распределения $a$ равно этому параметру распределения.

%\wdate {03.10.13}

	Если $\xi$ непрерывна, её закон распределения определяется плотностью распределения $ f_\xi(x) \ge 0 \Rightarrow M\xi =  \int\limits_{-\infty}^{\infty} x f_\xi(x) dx$. Если имеется функция $g(\xi), \; g(\xi,\eta)$, то математическое ожидание вычисляется по формулам:

	$$
	  M_{g_\xi} = \int\limits_{-\infty}^{\infty} g(x) f_\xi(x) dx
	$$

	$$
	  M(\xi,\eta) = \iint\limits_{-\infty}^{\quad\infty} g(x,y) f_{\xi\eta}(x,y) dx \, dy
	$$

	где $f(\xi,\eta)$ - плотность совместных случайных величин.

	Эти математические ожидания существуют, если все написанные несобственные интегралы сходятся.


	\task{Пример: } вычислить математическое ожидание $\xi$, равномерно распределённое по закону Пуассона
	
	\rtask{Решение: } 

		$$
		  f_\xi (x)= \frac{1}{b-a},
		    a \le x \le b \; | \; 0,x \in \mbox{\small в остальных случаях}
		$$

		$$
		  M_\xi = \int\limits_{-\infty}^\infty x f_\xi(x) dx
		        = \frac{1}{b-a} \int\limits_a^b x dx 
		        = \left. \frac{x^2}{2(b-a)} \right|^b_a 
		        = \frac{a+b}{2}
		$$


	\task{Пример 2: } вычислить математическое ожидание случайной величины $\xi$, распределённой нормально (по закону распределения Гаусса) 

		$$ f_\xi(x) = \frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x-a)^2}{2\sigma^2}} 
		            = \frac{1}{\sqrt{2\pi}\sigma} \int\limits^\infty_{-\infty} (x-a) e^{-\frac{(x-a)^2}{2\sigma^2}} dx 
		             +
		$$ $$		
		             + \frac{a}{\sqrt{2\pi}\sigma} \int\limits^\infty_{-\infty} (x-a) e^{-\frac{(x-a)^2}{2\sigma^2}} dx 	\; \mbox{\scriptsize (интеграл Лапласа)}
		            = 0 + \frac{a}{\sqrt{2\pi}\sigma} \sqrt{2\pi}\sigma 
		            = a
		$$

	\rtask{Вывод: } Распределение случайной величины, распределённой нормально, равно параметру распределения.





\subsubsection{Дисперсия случайной величины и её основные свойства.}

	Дисперсия $D\xi$ - число, определяемое формулой $D\xi = M(\xi-M\xi)^2 \; (1)$, т.е. дисперсия представляет собой квадрат разности случайной величины и её математического ожидания. Другое название - квадрат среднеквадратического отклонения. 

	Часто в прикладных задачах вместо $D$ рассматривают величину $\sqrt{\!D}$, называемую среднеквадратическим отклонением

	Формулу (1) можно продолжить, тогда мы получим 
	  $$
	    D_\xi = M\left(\xi^2- \strut 2\xi M\xi + (M\xi)^2\right) 
	          = M^2\xi - 2M\xi + 2M\xi + (M\xi)^2 
	          = M^2\xi - (M_\xi)^2,
	  $$
	откуда $(2) \quad D\xi = M^2\xi + (M\xi)^2$


	\begin{enumerate}
	
	\item Пусть $\xi$ - дискретная величина, принимающая значения $x_1, \sudots, x_n$ с вероятностями $p_1, \sudots ,p_n$

	$$
	  D\xi = \sum^n_{k=1}(x_k M\xi)^2*p_k = (2)
	       = \sum^n_{k=1}(x^2_k*p_k)^2 - (M\xi)^2
	$$

	\item Пусть $\xi$ - непрерывная случайная величина, значит может быть определена функция $f_\xi(x)$. 
	
	$$
	  D\xi = (1) = \int\limits_{-\infty}^\infty x^2 f_\xi(x) dx - (M\xi)^2
	$$

	\end{enumerate}


	Дадим механическую интерпретацию математического ожидания и дисперсии случайной величины. Будем представлять закон распределения вероятностей $p_k = P(\xi = x_k), \sum\limits^n_{k=1}p_k = 1$ случайной величины $\xi$, как закон распределения единичной массы на прямой: в точках $x_k$ сосредоточены массы $p_k$ : 
	
	$$
	  ---\frac{x_1}{p_1}---\frac{x_2}{p_2}- \cdot \cdot \cdot -\frac{x_n}{x_n}--->x
	$$
	
	Тогда 
	
	$$
	  M\xi = \sum^n_{k=1} x_k P(\xi=x_k) \mbox{ - центр тяжести СМАТ}	
	$$ 

	$$
	  D\xi = \sum^n_{k=1} (x_k - M_\xi)^2 * p_k \mbox{ - момент инерции относительно начала координат}
	$$

 	\task{Пример: } $D\xi = ?, \; f_\xi(x) = e^{-\frac{(x-a)^2}{2\sigma^2}}$

 	\rtask{Решение: } 
 	
 	$$
 	  D\xi = \int\limits^\infty_{-\infty}(x-M\xi)^2 f_\xi(x) dx
 	       = \frac{1}{\sqrt{2\pi}\sigma}\int\limits^\infty_{-\infty} (x-a)^2 e^{-\frac{(x-a)^2}{2\sigma^2}} dx
 	       = 
 	$$
 
 	Произведём замену переменной по формуле $y = \frac{x-a}{\sigma}, \; x - a = \sigma y, \; dx = \sigma dy$
 
 	$$
 	   = \frac{1}{\sqrt{2\pi}\sigma} \int\limits_{-\infty}^\infty \sigma^2 e^{-\frac{y^2}{2}} \sigma dy 
 	   = \frac{\sigma^2}{\sqrt{2\pi}}  \int\limits _{-\infty}^\infty (-y ) de^{-\frac{y^2}{2}} 
	%   =
 	% $$ $$
 	   = \frac{\sigma^2}{\sqrt{2\pi}} \left( \left. -ye^{-\frac{y^2}{2}} \right|^{+\infty}_{-\infty} \strut 
 	     + \int\limits_{-\infty}^\infty e^{-\frac{y^2}{2}} dy \right)
 	   = \sigma^2
 	 $$
 
	 Вывод: дисперсия нормального распределения случайной величины равна второму параметру распределения $(\sigma^2)$: $M_\xi = a; \; D_\xi = \sigma^2$

	


	
\subsubsection{Свойства дисперсии}


	\begin{enumerate}

	\item	Дисперсия неотрицательна: $D_\xi \ge 0.$ $D_\xi = 0 \Longleftrightarrow \xi = const$. 
		
		Доказательство: $D\xi = M(\xi-M\xi)^2 \ge 0 - $ по свойству монотонности.
		
		Пусть $\xi = c = const$.
		
		Тогда $D_c = M(c-M_c)^2 =(c - c)^2 = 0$.


	\item	Если $a = const$, то дисперсия $D(a\xi) = a^2 D\xi$. 
	
		Доказательство: 
		
		$D(a\xi) = M(a\xi - Ma\xi)^2
		         = M(a\xi - a M\xi)^2
		         = M[a^2 (\xi-M\xi)^2]
		         = a^2 M(\xi - M\xi)
		         = a^2 D\xi
		$
		

	\item	Если $\xi,\eta$ независимы, то 
		$$
		  D(\xi + \eta) = M\left(  \xi \strut + \eta - M(\xi +\eta)    \right)^2 
		                = M\left( (\xi - M\xi) \strut + (\eta - M\eta) \right)^2
		                =
		$$ $$		                
		                = M(\xi - M\xi)^2 + 2(\xi-M\xi)(\eta - M\eta) + M(\eta - M\eta)^2
		                =
		$$ $$
		                = M(\xi - M\xi)^2 + 2M\left( (\xi - M\xi) \strut * (\eta - M\eta)\right) 
		                  + M(\eta - M\eta)^2
		                = D\xi + D_\eta
		$$
	
	\end{enumerate}
	
	
	
	

\subsection{Энтропия и информация}

\subsubsection{Энтропия как мера неопределённости}

	Для практики важно уметь численно оценивать степень неопределённости самых разнообразных опытов, чтобы иметь возможность их сравнивать. 
	
	Начнём с рассмотрения опытов имеющих $K$ равновероятных исходов. Степень неопределённости каждого такого опыта определяется числом $K$. При $K = 1$ исход опыта не является случайным. При большом значении $K$ предсказание результата опыта становится затруднительным.
	
	Таким образом, искомая численная характеристика степени неопределённости должна зависть от $K$, т.е быть функцией $f(k); \; f(1)=0$; при возрастании аргумента, функция должна возрастать. Для более полного определения функции $f(k)$ необходимо предъявить к ней дополнительные требования. 
	
	Рассмотрим сложный опыт $\alpha\beta$, состоящий в одновременном выполнении опытов $\alpha$ и $\beta$. Неопределённость выполнения сложного опыта больше неопределённости опыта $\alpha$, т.к. к его неопределённости надо добавить неопределённость опыта $\beta$. Поэтому естественно считать, что \term{степень неопределённости} опыта $\alpha\beta$ равна сумме неопределённостей, характеризующих $\alpha$ и $\beta$.
	
	Пусть $\alpha \beta$ имеет $k*l$ равновероятных исходов, $k\alpha, \; l\beta$. Приходим к следующему условию, которму должна удовлетворять функция $f(kl) = f(k) + f(l)$. Последнее условие наталкивает на мысль принять за меру неопределённости опыта, имеющего K равновероятных исходов число $\log k$: $\log(kl) = \log k + \log l$. Такое определение меры неопределённости согласуется с первоначальными условиями, что $f(1) = \log1 = 0; f(k) \mbox{ - возрастающая функция}$. Можно доказать, что логарифмическая функция является единственной, удовлетворяющей этим условиям.

%/next

 	\comm{Замечание: } отметим, что выбор основания логарифма большой роли не играет, поскольку в силу известной формулы перехода можем написать $\log_ba = \log_ca/\log_cb \Rightarrow \log_bk = \log_ba*log_ak$ сводится к домножению на константу, т.е. равносилен простому изменению \term{единицы измерения} степени неопределённости. Обычно за меру степени неопределённости берут логарифмы при основании 2: $log_2k = logk$, причём основание 2 не фиксируют. Т.е. за единицу измерения степени неопределённости принимают неопределённость опыта, имеющего 2 равновероятных исхода: $\log_22 = 1$ бит. Везде далее будем пользоваться двоичными единицами измерения. 

	Таблица вероятности для опыта, имеющего $K$ равновероятных исходов:
	

	\begin{tabular}{|l|c|c|c|c|}
	\hline
		$\alpha$ &&&&\\
	\hline
		Исходы & $A_1$ & $A_2$ & $\sudots$ & $A_k$ \\
	\hline
		Вероятности& $\frac{1}{k}$& $\frac{1}{k}$ & $\sudots$ & $\frac{1}{k}$\\
	\hline
	\end{tabular}


	Поскольку при наших допущениях неопределённость равна $f(k) = \log k$. В этом случае каждый отдельный исход вносит неопределённость $\frac{1}{k}$. $\frac{\log k}{k}= \frac{1}{k} \log k = -\frac{1}{k}\log\frac{1}{k}$.
	
	В самом общем случае опыт имеет следующую таблицу вероятности:


	\begin{tabular}{|l|c|c|c|c|}
	\hline	
		$\alpha$&&&&\\
	\hline
		Исходы & $A_1$ & $A_2$ & $\sudots$ & $A_k$ \\
	\hline
		Вероятности & $P(A_1)$ & $P(A_2)$ & $\sudots$ & $P(A_k)$ \\
	\hline
	\end{tabular}

	
	Для опыта общая мера неопределённости равна 
	  $ -p(A_1)  \log p(A_1) - p(A_2)  \log p(A_2) - \sudots - p(A_k)  \log p(A_k) = H(\alpha) \mbox{ - энтропия опыта $\alpha$}$

	Рассмотрим некоторые свойства энтропии $H(\alpha)$:


	\begin{enumerate}
	
	\item $H(\alpha) \ge 0$ 

	Доказательство:
	
	$ -p(A) \log p(A) \ge 0 \mbox{ (\scriptsize множители $\in$ промежутку $(0 \le p(A) \le 1)$ ) } $

	$ -p(A) \log p(A) = 0 \Longleftrightarrow \{ p=0; p=1 \}$

	В случае, если опыт имеет $K$ попарно несовместных исходов, то $H(\alpha) = 0$ равносильно тому, что один исход - достоверное событие, а все другие - невозможны, так как $\left(p(A_1)\strut+\sudots+p(A_k) = 1\right)$.	Это обстоятельство хорошо согласуются с величиной $H(\alpha)$ -  только в этом случае опыт вообще не содержит неопределённости.


	\item Из всех опытов c $K$ исходами самым неопределённым является опыт опыт с $K$ равновероятными исходами. Можно показать, что имеет место неравенство 
	
	$H(\alpha) = - p(A_1) \log p(A_1) - \sudots - p(A_k) \log p(A_k) \le H(\alpha_0)$
	
	$H(\alpha_0) = \log k = -\frac{1}{k} - \sudots -\frac{1}{k}$. 
	
	Равенство достигается при равных вероятностях $P(A_i); \; i = [\overline{1,k}]$
	
	\end{enumerate}


	\task{Пример: } Имеется две урны с 20-ю шарами каждая. Первая - 10 белых, 5 чёрных, 5 красных. Вторая - 8 белых, 8 чёрных, 4 красных.

	Из каждой урну вынимают по 1 шару. Исход какого из двух опытов следует считать более неопределённым?
	
	\rtask{Решение: } Обозначим опыты как А1 и A2.

	A1

	\begin{tabular}{|l|c|c|c|}
	\hline
		Исходы & Бел & Чёр & Крас \\
	\hline
		Вероятности & 1/2 & 1/4 & 1/4 \\
	\hline
	\end{tabular}

	A2

	\begin{tabular}{|l|c|c|c|}
	\hline
		Исходы & Бел & Чёр & Крас \\
	\hline
		Вероятность & 2/5 & 2/5 & 1/5 \\
	\hline
	\end{tabular}

	Энтропия опыта A1: 
		$H(\alpha_1)= - \frac{1}{2} \log \frac{1}{2} 
		              - \frac{1}{4} \log \frac{1}{4}
		              - \frac{1}{4} \log \frac{1}{4}
		            = - \frac{1}{2} * 1 
		              - \frac{1}{2} * (-2) 
		            = - \frac{1}{2} + 1
		            = 1,5 \mbox{бита}$.
	
	Энтропия опыта A2:
		$H(\alpha_2)= - \frac{2}{5} \log \frac{2}{5}
		              - \frac{2}{5} \log \frac{2}{5}
		              - \frac{1}{5} \log \frac{1}{5}
		            = - \frac{4}{5}(\log 2-\log 5) - \frac{1}{5} (\log 1 - \log 5)
		            = - 0.8 + - \frac{4}{5} \log 5 + \frac{1}{5} \log 5 
		            = - 0.8 + \log 5 = 1,52 \mbox{ бита.}
		$

	\rtask{Вывод: } Если оценивать степень неопределённости опыта его энтропией, то исход второго опыта более неопределённый, нежели первого.
	
	\stitle{Историческая справка} % =)

	Исторически первые шаги к введению понятия энтропии были сделаны в 1928 году американским инженером-связистом Хартли, предложившим характеризовать степень неопределённости опыта c К различными исходами числом $\log k$. Предложенная им мера степени неопределённости иногда бывает удобна в некоторых практических задачах, но часто оказывается малопоказательной, поскольку полностью игнорирует различие между характером имеющихся исходов. Поэтому почти невероятному исходу у Хартли придаётся такое-же значение, как и исходу весьма вероятному. Однако, он считал, что различия между отдельными исходами определяются в первую очередь "психологическими факторами" и должны учитываться лишь психологами, но не инженерами или математиками.
	
	Ошибочность точки зрения Хартли была показана другим американским инженером~-~математиком К. Шенноном. Он предложил принять в качестве меры неопределённости опыта с $K$ различными исходами $A_1,\sudots,A_k$ величину
	
	 $H(\alpha) = - p(A_1) \log p(A_1) - \sudots - p(A_k \log p(A_k)$. 
	
	Иначе говоря, исходу $A_i$ следует приписать неопределённость, равную $- \log p(A_i)$. В качестве неопределённости всего опыта $H(\alpha)$ принимается среднее значение случайной величины (математическое ожидание), равное $H(\alpha) \xi $ ,где $\xi \mbox{ принимают значения } - \log p(A_i) \mbox { с вероятностями } p(A_i)$.

	Таким образом, загадочные "психологические факторы" учитываются с помощью использования понятия вероятности, имеющего чисто математический, а точнее статистический характер.
	
	Использование величины $H(\alpha)$ в качестве меры неопределённости опыта $A$ оказалось полезным во многих областях, а особенно в теории передачи сообщений по линиям связи.

\subsubsection{Энтропия сложных событий. Условная энтропия}

	Условная энтропия. Пусть имеются два независимых опыта $A, B$ с таблицей вероятностей $A_1, p(A_1); \sudots ;A_k, p(A_k); \quad B_1, p(B_1); \sudots ;B_l, p(B_l)$.
	
	Рассмотрим сложный опыт $\alpha \beta$, когда осуществляются оба опыта одновременно, имеющий $k*l$ исходов ($A \times B$ - декартово произведение).

	$A_1B_1 : \alpha = A_1; \beta = B_1$

	Очевидно, что неопределённость опыта $\alpha \beta$ больше неопределённости каждого из опытов, из-за осуществления обоих опытов. Поэтому имеет место соотношение $H(\alpha\beta) = H(\alpha)+H(\beta)$. Написанное равенство называется правилом сложения энтропии для опытов $\alpha$ и $\beta$.

	Для доказательства этого равенства рассмотрим выражение 
	
	$$
	  H(\alpha\beta) = 
	    - p(A_1B_1) \log p(A_1 B_1) 
	    - \sudots 
	    - p(A_kB_l) \log p(A_k B_l)
	$$ 
	
	$$
	 \alpha, \beta \mbox{ - независимы, следовательно } 
	   p(A_iB_j) = p(A_i) * p(B_j) \Rightarrow
	$$
	
	$$
	  \log p(A_iB_j) 
	   = \log p(A_i) p(B_j) 
	   = \log p(A_i) + \log p(B_j).
	$$



%\wdate{17.10.13}

	%Реферат - 5-10стр.
	
	Предположим далее, что $\alpha$ и $\beta$ - зависимые опыты (пример: $\alpha$, $\beta$ - последовательные извлечения двух шаров из одной урны.) Постараемся выяснить, чему равна энтропия сложного опыта $\alpha \beta$ в этом случае. 
	
	Здесь уже нельзя заменить $p(A_1 B_1) , p(A_1 B_2), \sudots $ произведением вероятностей, а необходимо использовать условную вероятность $p(A_1 B_1) = p(A_1) * p(B_1|A_1)$
	
	В этом случае можно доказать следующую формулу:
	
	$
	  H(\alpha\beta) = H(\alpha) +
	   \left[ 
	      p(A_1) * H(\beta|A_1) 
	    + p(A_2) * H(\beta|A_2)
	    + \sudots 
	    + p(A_k) * H(\beta|A_k) 
	   \right] 
	 (*)
	$ , где $H(\beta|A_i)$ - условная энтропия опыта $\beta$ при условии, что значение опыта $\alpha$ равно $A_i$.

	$
	  H(\beta|A_i) = 
	   - p(B_1|A_i) \log p(B_1|A_i) 
	   - p(B_2|A_i) \log p(B_2|A_i)
	   - \sudots 
	   - p(B_e|A_i) \log p(B_e|A_i) 
	  (**) 
	$
	
	Это выражение представляет собой энтропию опыта $\beta$ при условии, что имеет место событие $A_i$.
	
	$$
	\left\{
	 \begin{aligned}
	   H(\alpha,\beta) & = H(\alpha) + H(\beta) \quad (\mbox{ для независимых } \alpha, \beta)\\
	   H(\alpha,\beta) & = H(\alpha) + [\sudots](*) \quad (\mbox{ для зависимых }   \alpha, \beta)
	 \end{aligned}
	\right.
	$$

	Первый член последнего выражения $(*)$ - энтропия опыта $\alpha$. Что же касается второго - он есть математическое ожидание случайной величины, принимающей с вероятностями $p(A_1), \sudots ,p(A_k)$ значения $H(\beta | A_1), \sudots, H(\beta | A_k)$, то есть значения, равные условной энтропии опыта $\beta$, при условии, что опыт $\alpha$ имеет исходы $\alpha : A_1, \sudots, A_k$. Это среднее значение естественно назвать \term{условной энтропией} выполнения опыта $\beta$ при условии выполнения опыта $\alpha$,

	$$ 
	  H(\beta|\alpha) = [\sudots](*) = p(A_1) H(\beta|A_1) + p(A_2) H(\beta|A_2) + \sudots + p(A_k) H(\beta|A_k)
	$$

	Тогда соотношение $(*)$ переписывается как $H(\alpha\beta) = H(\alpha) + H(\beta|\alpha) (*);$ $ \alpha, \beta \mbox{ - зависимы}$.

	Это и есть общее правило для определения энтропии сложного опыта $\alpha\beta$. Его также можно назвать правилом сложения энтропии, для \term{зависимых} опытов $\alpha\beta$.
	
	Укажем основные свойства условной энтропии:


	\begin{enumerate}
	
	\item	$H(\beta|\alpha) \ge 0$.
	
	\item	$p(A_1), \sudots, p(A_k) \not = 0$ (опыт имеет к штук исходов).
		
		Тогда $H(\beta|\alpha) = 0 \Longleftrightarrow H(\beta|A_1) = \sudots = H(\beta|A_k) = 0$, т.е. при любом исходе опыта $\alpha$ результат опыта $\beta$ полностью определён, и при этом имеем 
		$H(\alpha\beta) = H(\alpha)$.
		
		Если $\alpha$ и $\beta$ \term{независимы}, то тогда $H(\beta|\alpha) = H(\beta)$, и $H(\alpha\beta) = H(\alpha)+H(\beta)$.
	
	\item	Во всех случаях условная энтропия $H(\beta|\alpha)$ заключается между $0$ и $H(\beta)$:	
		
		$0 \le H(\beta|\alpha) \le H(\beta)$. 

		Таким образом случаи, когда исход $\beta$ полостью предопределяется исходом $\alpha$ и когда опыты $\alpha$ и $\beta$ независимы, являются в определённом смысле крайними.	

	\item 	Условная энтропия. 
		$$
		  H(\alpha\beta) = H(\beta\alpha) \Rightarrow 
		    H(\alpha) + H(\beta|\alpha) 
		     = H(\beta) + H(\alpha|\beta) \Rightarrow	     
		$$ $$ \Rightarrow
		    H(\beta|\alpha)
		     = H(\alpha|\beta)(.) + H(\beta) - H(\alpha)
		$$
		
		$H(\alpha|\beta) = 0$ (исход опыта $\beta$ полностью определяет опыта $\alpha$
		
		$H(\beta|\alpha) = H(\beta) - H(\alpha)$
		
	\end{enumerate}

	
	\task{Задача: } %(будет на зачёте)
		Задача о болезненной реакции. 
		
		Известно, что некоторой болезнью в среднем болеют 2 человека из 100. Для выявления больных используется определённая реакция, которая всегда оказывается положительной в том случае, когда человек болен. Если же человек здоров, то она столь же часто бывает положительной, как и отрицательной. Пусть опыт $\beta$ состоит в определении того болен или здоров человек, а опыт $\alpha$ - в определении результата указанной реакции. Спрашивается, какова будет энтропия $H(\beta) = ?$ опыта $\beta$ и условная энтропия $H(\beta|\alpha) = ?$.
	
	\rtask{Решение:} Очевидно, что $\beta$ имеет 2 исхода: $\beta : \{B_1\mbox{ - здоров}; \; B_2\mbox{ - болен}\}$.
	
	$p(B_1) = 0.98; \quad p(B_2) = 0.02$.
	
	$H(\beta) = - 0.98 \log 0.98 - 0.02 \log 0.02 \approx 0.14 \mbox{бит}.$
	
	$H(\beta) \approx 0.14 бит.$
	 
	Рассмотрим опыт $\alpha$: \; $\alpha : {A_1 - \mbox{ положительная реакция}; \; A_2 - \mbox{ отрицательная реакция}}$

	$  p(A_1) = p(\frac{B_1}{2} + B_2) 
	          = p(\frac{B_1}{2}) + p(B_2)
	          = 0.49 + 0.02
	          = 0.51
	$.

	$  p(A_2) = p(\frac{B_1}{2}) 
	          = 0.49
	$.

	$
	  \alpha = A_1 : p(B_1|A_1)
	         = \frac{p(B_1 A_1)}{p(A_1)}
	         = \frac{0.49}{0.51}
	         = \frac{49}{51}.
	$
	
	$
	  \alpha = A_2 : p(B_2|A_1) 
	         = \frac{p(B_2 A_1)}{p(A_1)} 
	         = \frac{0.02}{0.51} 
	         = \frac{2}{51}. 
	$
	
	Пользуясь этими данными мы можем найти условную энтропию $H(\beta)$ при выполнении события $A_1$

	$$
	  H(\beta|A_1) = - \frac{49}{51} \log \frac{49}{51}
	                 - \frac{2}{51}  \log \frac{2}{51}
	         \approx 0.24 \mbox{ бит.}
	$$
	
	При $\alpha = A_2 \Rightarrow \beta = B_1; \; H(\beta|A_2) = 0$, т.е. мы с уверенностью можем утверждать, что человек здоров, и опыт $\beta$ имеет исход $B_1$.
	
	Таким образом, условная энтропия $\beta$ при условии осуществления $\alpha$ будет равна 
	
	$$
	  H(\beta) ~= 0.14 бит | sys 	H(\beta|A_1) \approx 0.045
					H(\beta|A_2) = 0 \sim \sim
	$$
				
	$$
	  H(\beta|\alpha) = p(A_1)H(\beta|A_1) + p(A_2)H(\beta|A_2) 
	            \approx 0.51 * 0.24 + 0.49 * 0
	                  = 0.12 \mbox{ бит.}
	$$
	
	Иначе говоря, выполнение опыта $\alpha$ уменьшает неопределённость опыта $\beta$ на $0.002$ бита.
	
	
	
	\subsubsection{Понятие об информации.}
	
	Вернёмся вновь к величине $H(\beta)$, характеризующей степень неопределённости опыта $\beta$. Равенство этой величины $0$ означает, что исход опыта $\beta$ заранее известен. Большее или меньшее значение числа $H(\beta)$ отвечает большей или меньшей проблематичности определения результата опыта $\beta$. Какое-либо измерение или наблюдение в виде опыта $\alpha$, предшествующее $\beta$ может ограничить количество возможных исходов опыта $\beta$, и тем самым уменьшить степень его неопределённости: так, к примеру степень неопределённости опыта, состоящего в нахождении самого тяжёлого из $3$ грузов уменьшается после сравнения на весах двух из них. 

	Для того, чтобы результат измерения(наблюдения) $\alpha$ мог сказаться на последующем опыте $\beta$ необходимо, чтобы $\alpha$ \term{не был известен заранее.} Поэтому, $\alpha$ можно рассматривать как вспомогательный опыт, также имеющий несколько допустимых исходов. 
	
	Тот факт, что осуществление $\alpha$ уменьшает степень неопределённости $\beta$ отражается в неравенстве, где условная энтропия $H(\beta|\alpha) \le H(\beta)$ первоначальной энтропии опыта $\beta$.

%\next

	При этом, если опыт $\beta$ не зависит от $\alpha$, то осуществление $\alpha$ не уменьшает энтропии $\beta$. Это значит, что $H(\beta|\alpha) = H(\beta)$. Если же результат $\alpha$ полностью предопределяет исход опыта $\beta$, то энтропия $\beta$ уменьшается до $0$: $H(\beta|\alpha) = 0$. Таким образом, разность \fbox{$I(\beta,\alpha) = H(\beta) - H(\beta|\alpha) (*)$}.

	Таким образом написанная разность указывает, насколько осуществление $\alpha$ уменьшает неопределённость $\beta$, т.е. как много мы узнаём об исходе опыта $\beta$, произведя измерение(наблюдение) в виде опыта $\alpha$. Эта разность $(*)$ называют количеством информации относительно опыта $\beta$, содержащейся в опыте $\alpha$. Таким образом, мы получаем возможность \term{численного измерения} информации. К примеру, в условиях задачи о болезненной реакции можно сказать, что используемая реакция в виде опыта $\alpha$ даёт информацию о заболевании в виде опыта $\beta$, равное $0.14-0.12 = 0.02$ бита. Эта цифра и оценивает пользу реакции.
	
	Соотношение между понятиями энтропии и информации напоминает соотношение между физическими понятиями потенциала и разности потенциалов. Энтропия есть абстрактная мера неопределённости. Ценность этого понятия в значительной мере заключается в том, что оно позволяет оценить влияние на опыт $\beta$ какого-либо другого опыта $\alpha$ как разность энтропий по формуле $(*)$.
	
	Подчеркнём также, что информация относительно опыта $\beta$, содержащаяся в опыте $\alpha$ представляет собой среднее значение (математическое ожидание) случайной величины $H(\beta) - H(\beta|A_i)$, связанной с отдельными исходами $A_i$ опыта $\alpha$.
	
	\task{Пример:} Задача о шарах и предварительной информации.
	
	% Конспектирование этих формул в реалтайме - просто угар \right)
	
	Пусть опыт $\beta$ состоит в извлечении одного шара из урны:
	
	$\beta$ : 1 шар из 5 чёрных и 10 белых. 

	А опыт $\alpha_k$ состоит в предварительном извлечении (без возвращения обратно) $K$ шаров:
	
	$\alpha_k$ : {\scriptsize $K$ шаров извлечено.}
	
	$H(\beta) = \; ?$

	$I(\beta,\alpha_1) = \; ?$
	
	$I(\beta,\alpha_2) = \; ?$

	$I(\beta,\alpha_{13}) = \; ?$

	$I(\beta,\alpha_{14}) = \; ?$

	Чему равна энтропия $H(\beta)$ и информация, содержащаяся в опыте $\alpha_1$? 
	
	\rtask{Решение:} 
	
	$
	  H(\beta) = - \frac{1}{3} \log \frac{1}{3} 
	             - \frac{2}{3} \log \frac{2}{3} 
	       \approx 0.92 \mbox{бита}. 
	$

	\strut

	$	
	  \; I(\beta,\alpha_1) = H(\beta) - H(\beta|\alpha_1)
	$

	$
	  \begin{aligned}
	  H(\beta|\alpha_1) = 
	&
	            \left[  p(A_1^{\mbox{\scriptsize чёр}}) * H(\beta|A_1^{\mbox{\scriptsize чёр}})
	           \right] 
	                  + 
	            \left[  p(A_1^{\mbox{\scriptsize бел}}) * H(\beta|A_1^{\mbox{\scriptsize бел}})
	           \right]
	     =
	\\
	     = 
	&
	       - \frac{1}{3} 
	           \left[
	             \underbrace{p(B^{\mbox{\scriptsize чёр}} | A_1^{\mbox{\scriptsize чёр}})}_{4/14} * \log \frac{4}{14} 
	           + \underbrace{p(B^{\mbox{\scriptsize бел}} | A_1^{\mbox{\scriptsize чёр}})}_{5/7} * \log \frac{5}{7}
	          \right]
	       -
	\\
	&
	       - \frac{2}{3}
	           \left[
	             \underbrace{p(B^{\mbox{\scriptsize чёр}} | A_1^{\mbox{\scriptsize бел}})}_{5/11} * \log \frac{5}{11}
	           + \underbrace{p(B^{\mbox{\scriptsize бел}} | A_1^{\mbox{\scriptsize бел}})}_{9/14} * \log \frac{9}{14}
	          \right] 
	  \approx 0.004 \mbox{бит}.
	  \end{aligned}	
	$
	
	\strut

	$
	  \; I(\beta|\alpha_2) = H(\beta) - H(\beta|\alpha_2)
	$ 
	
	$
	  \begin{aligned}
	  H(\beta|\alpha_2) = 
	& \;
	     % \left[
	              p(A_1^{\mbox{\scriptsize ч}} A_2^{\mbox{\scriptsize ч}}) 
	               * H(\beta | A_1^{\mbox{\scriptsize ч}} A_2^{\mbox{\scriptsize ч}})
	     % \right]
	     +
	     % \left[
	              p(A_1^{\mbox{\scriptsize ч}} A_2^{\mbox{\scriptsize б}})
	               * H(\beta | A_1^{\mbox{\scriptsize ч}} A_2^{\mbox{\scriptsize б}})
	     % \right]
	     +
	     % \left[
	              p(A_1^{\mbox{\scriptsize б}} A_2^{\mbox{\scriptsize б}})
	               * H(\beta | A_1^{\mbox{\scriptsize б}} A_2^{\mbox{\scriptsize б}})
	     % \right]
	     =	
	\\
	     = - \left\{ 
	                \vphantom{\underbrace{p(A_1)}_{3/13}}  
	                \frac{C^2_5}{C^2_{15}} 
	         \right.
	&
	         \left[  \underbrace{ p(B^{\mbox{\scriptsize чёр}} |
	                                A_1^{\mbox{\scriptsize чёр}} A_2^{\mbox{\scriptsize чёр}})}_{3/13}
	                  * \log \frac{3}{13}
	              +
	                 \underbrace{ p(B^{\mbox{\scriptsize бел}} |
	                                A_1^{\mbox{\scriptsize чёр}} A_1^{\mbox{\scriptsize чёр}})}_{10/13}
	                  * \log \frac{10}{13}
	         \right]
	       +
	\\
	       + \frac{C^1_{10} C^1_5}{C^2_{15}}
	&
                  \left[  \underbrace{ p(B^{\mbox{\scriptsize чёр}} |
                                         A_1^{\mbox{\scriptsize чёр}} A_2^{\mbox{\scriptsize бел}})}_{4/13}
                           * \log \frac{4}{13}
	               +
	                  \underbrace{ p(B^{\mbox{\scriptsize бел}} |
	                                 A_1^{\mbox{\scriptsize чёр}} A_2^{\mbox{\scriptsize бел}})}_{9/13}
	                   * \log \frac{9}{13}
	         \right]
	       +
	\\
	       + \frac{C^2_{10}}{C^2_{15}}
	&
	       \left.
                  \left[  \underbrace{ p(B^{\mbox{\scriptsize чёр}} |
                                         A_1^{\mbox{\scriptsize бел}} A_2^{\mbox{\scriptsize бел}})}_{5/13}
                           * \log \frac{5}{13}
	               +
	                  \underbrace{ p(B^{\mbox{\scriptsize бел}} |
	                                 A_1^{\mbox{\scriptsize бел}} A_2^{\mbox{\scriptsize бел}})}_{8/13}
	                   * \log \frac{8}{13}
	         \right]
	       \right\}
	    \approx 0.008 \mbox{бит}.
	\end{aligned}	
	$

	\strut \pagebreak[3]

	$
	   I(\beta,\alpha_{13}) = H(\beta) - H(\beta|\alpha_{13})
	$
	
	$H(\beta|\alpha_{13})$ - здесь неопределённость только в оставшихся двух шарах: они должны быть двух цветов, значит, мы взяли 4 чёрных и 9 белых шаров.

	
	$$ \hspace{-3em} %костыль
	  H(\beta|\alpha_{13}) = \frac{C^4_5 C^9_{10}}{C^{13}_{15}} (-) \left(  \frac{1}{2} \log \frac{1}{2} 
	                                                                    + \frac{1}{2}\log\frac{1}{2})
	                                                              \right)
	                     = - \frac { \frac{5!} {4!(5-4)!} \frac{10!} {9!(10-9)!} } 
	                               { \frac{15! }{13!(15-13)! * (-1)} }
	                     =   \frac{2*5*10}{14*15}
	                \approx 0.44 бит
	$$

	\strut

	$
	  I(\beta,\alpha_{14}) = \left[ H(\beta|\alpha_14) = 0 \right]
	               \approx 0.92 бит.
	$
	
	
	
	%\wdate{31.10.13}
	
	
	Вообще, надо сказать, что количество информации об опыте $\beta$: $I(\beta\alpha)$, которая заключается в опыте $\alpha$ является объективной характеристикой \term{ценности прогноза}. $H(\beta|\alpha) = H(\beta)$, если $\alpha$, $\beta$ независимы, или если $H(\beta) = 0$ (исход $\beta$ известен заранее и не нуждается  в прогнозе). Во всех остальных случаях имеем $0 < I(\beta|\alpha) \leq H(\beta)$

	Рассмотрим ситуацию, когда опыт $\beta$ имеет \term{бесконечное} число исходов 	(непрерывное множество исходов). В этом случае $H(\beta) = \infty$, однако вместо неё часто можно рассматривать \term{конечную энтропию} $H_\epsilon \beta$, которая получается при объединении исходов $\beta$, отличающихся не более, чем на малое число $\epsilon$, в один исход.
	В практических задачах обычно $H_\epsilon$ ($\epsilon$ - энтропия) и имеет смысл, так как мы вообще не можем различить исходы $\beta$, отличающиеся меньше, чем на $\epsilon$. ($\epsilon$ определяется точностью используемых измерительных приборов)
	
	Информация $I(\beta,\alpha) = I(\alpha,\beta)$. Проверим: Очевидно равенство 
	
	$              H(\alpha\beta) = H(\beta\alpha) 
	  \Rightarrow  H(\alpha) + H(\beta|\alpha) = H(\beta) + H(\alpha|\beta)
	  \Rightarrow  H(\alpha) + H(\alpha|\beta) = H(\beta) + H(\beta|\alpha)
	  \Rightarrow I(\alpha|beta) = I(\beta|\alpha)
	$

	Таким образом, информацию $I(\beta,\alpha)$ можно назвать \term{взаимной информацией} опытов $\alpha$ и $\beta$ друг относительно друга.
	
	Пусть $\alpha, \beta, \gamma$ - 3 произвольных опыта. В таком случае всегда $I(\beta,\alpha\gamma) \ge I(\beta,\alpha)$
	
	Иначе говоря, опыт $\alpha\gamma$ содержит не меньше информации, чем простой опыт $\beta$
	 
	Про \term{последовательной} передаче информации об опыте $\alpha$, осуществляемой посредством цепочки опытов $\beta, \gamma, \delta \sudots$, где только опыт $\beta$ непосредственно связан с $\alpha$,а $\gamma$ всю содержащуюся информацию об $\alpha$ получает из связи с опытом $\beta$ (так, что $\beta\gamma$ не содержит дополнительной информации по сравнению с опытом $\beta$); $\delta$ всю информацию получает из связи с опытом $\gamma \sudots$
	 
	Информация об опыте $\alpha$ в этом случае может только уменьшаться. 
	$$
	  H(\alpha) = I(\alpha,\alpha) 
	              \ge I(\alpha,\beta)
	              \ge I(\alpha,\gamma)
	              \ge I(\alpha,\delta)
	              \ge \sudots
	$$
	Наглядной иллюстрацией этого положения может служить игра "испорченный телефон".
	 	 
	Величина $I_\beta(\alpha,\gamma) = H(\alpha|\beta) - H(\alpha|\beta\gamma)$ называется условной информацией двух опытов $\alpha$ и $\gamma$ друг от друга при выполнении опыта $\beta$.
	
	Свойства условной информации:


	\begin{enumerate}
	
	\item	$I_\beta(\alpha,\gamma) \ge 0$
	
	\item	$I_\beta(\alpha,\gamma) = I_\beta(\gamma,\alpha)$ (симметричность).

	\item	$I(\alpha,\beta\gamma) = I(\alpha,\beta) + I_\beta(\alpha,\gamma)$
	
		Получается из формул 
		
		$I_\beta(\alpha,\gamma) =  H(\alpha|\beta) - H(\alpha|\beta\gamma)$
	
		$I(\alpha,\beta\gamma) = H(\alpha) - H(\alpha|\beta\gamma)$
		
		$I(\alpha,\beta) = H(\alpha) - H(\alpha|\beta)$
	
	\end{enumerate}	




	
	\subsubsection{Определение энтропии перечислением её свойств}
	
	Понятие энтропии с необходимостью вытекает из простейших требований, которые естественно наложить на величину, служащую количественной характеристикой степени неопределённости. Энтропия (мера степени неопределённости опыта $\alpha$) определяется с помощью таблицы вероятностей. 
	

	$\alpha$
	
	\begin{tabular}{|l|c|c|c|}
	\hline
	Исходы опыта & $A_1$ & \sudots & $A_k$ \\
	\hline
	Вероятности  & $p(A_1)$ & \sudots & $p(A_k)$\\
	\hline
	\end{tabular}
	
	
	$H(\alpha) = H(p_1,\sudots , p_k)$
	
	$p(A_1) = p_1, \sudots , p_k$
	
	Сформулируем те условия. выполнение которых надо требовать от функции энтропии:
	
	\begin{enumerate}
	
	\item	$H(p_1, \sudots ,p_k)$ не меняется при любой перестановке чисел $p_1 \sudots p_k$

	\item	$H(p_1, \sudots ,p_k)$ непрерывна, т.е она мало изменяется при малых изменениях вероятностей $p_k$. В самом деле, при малых изменениях вероятностей и степень неопределённости опыта должна мало изменяться

	\item	Функция $H(p_1,...p_k)$ удовлетворяет соотношению 
		$
		  H(\alpha) = H(p_1,p_2,...,p_k) 
		            = \overbrace{H(p_1+p_2,p_3,...,p_k)}^{H(\beta)} + (p_1+p_2) 
		            * H(\frac{p_1}{p_1+p_2}б\frac{p_2}{p_1+p_2})
		$
	Это значит, что неопределённость опыта $\beta$ с таблицей вероятностей,
	
		\begin{tabular}{|l|c|c|c|}
		\hline
		Исходы опыта & $B$ & \sudots & $A_k$\\
		\hline		
		Вероятности  & $p_1+p_2$ & \sudots & $p(A_k)$\\
		\hline
		\end{tabular}
		
	получаемая отождествлением первых двух исходов опыта $\alpha$ меньше неопределённости последнего опыта, умноженную на меру неопределённости опыта $p_1+p_2$, состоящую в выяснении того, какой именно из первых исходов опыта $\alpha$ имел место.
	\end{enumerate}		 

	%\next

	%9^{00} 14 ноя Большая Зеленина, 7 кв 45, 6 эт. 8 96 5 03 9 43 24 ВТ 331 с 11 утра
	
	Можно доказать, что условия $1,2,3$ полностью определяют вид функции $H(p_1,p_2, \sudots ,p_k)$: единственная функция, удовлетворяющая этим условиям имеет вид $H(p_1,p_2, \sudots ,p_k) = -p_1\log p_1 - ... - p_k \log p_k$
	Следуя Шеннону, условия $1,2,3$ дополняют ещё одним условием : вводят в рассмотрение функцию $H(\frac{1}{k}, \sudots ,\frac{1}{k}) = H(\alpha_0)$ (опыт $\alpha_0$ имеет $K$ равновероятных исходов). 
	
	Очевидно, что в силу равновероятности исходов, функция зависит лишь от числа $K$. Также ясно, что степень неопределённости опыта $\alpha_0$ должна быть тем больше, чем больше число $K$ его исходов. Тогда можно утверждать, что
	
	
	\begin{enumerate}

	\item[4] $H(1\k,...1\k) = f(k)\nearrow$ (растёт)
	
	\end{enumerate}		
	


	
	\pagebreak
	
	\section{Процессы кодирования}
	
	
	
	
	\subsection{Различные виды кодов и их характерные особенности}
	
	
	
	
	\subsubsection{Основные понятия}
	
	Рассмотрим прежде всего \term{общую схему} передачи сообщений по линиям связи. Для определённости будем говорить о телеграфии. На одном конце линии отправитель подаёт некоторое сообщение, записанное при помощи 33 букв русского алфавита (ъ = ь, е = ё), но включая сюда и нулевую букву - промежуток между словами, или с помощью 27 букв латинского алфавита, или при помощи 10 цифр(цифровое сообщение)
	
	Для передачи этого сообщения в случае обычного проводного телеграфа используется постоянный ток, некоторые характеристики которого телеграфист может менять по своему усмотрению. При этом он создаёт определённую \term{последовательность} сигналов, воспринимаемых вторым телеграфистом на другом конце линии. Простейшими различимыми элементарными сигналами, широко используемыми на практике являются посылка тока и пауза (отсутствие посылки). При помощи только двух этих сигналов можно передать любое сообщение, если условиться заменять каждую букву или цифру определённой комбинацией посылок тока и пауз. В технике связи правила, сопоставляющие каждому передаваемому сообщению  некоторую комбинацию сигналов, называется \term{кодом} (телеграфным), а сама операция перевода сообщения в последовательность различимых сигналов называется \term{кодированием} сообщения. При этом коды, использующие только 2 различных элементарных сигнала называются \term{двоичными} кодами. Коды, использующие 3 различных сигнала, называются троичными и.т.д.
	
	В телеграфии применяется целых ряд различных кодов, важнейшими из которых являются код Морзе, код Бодо.
	
	В коде Морзе каждой букве/цифре сообщения сопоставляется некоторая последовательность кратковременных посылок тока (точек) и втрижды более длинных посылок тока (тире), разделяемых кратковременными паузами той-же длительности, что и точки. Пробел отмечается специальным разделительным знаком - длинной паузой, длинной с тире, а пробел между словами - ещё вдважды более длинной паузой. 
	
	В настоящее время код Морзе используется лишь при повреждении основных телеграфных линий, а также в КВ-радиотелеграфии.
	
	Код Бодо: В обычных буквопечатающих телеграфных аппаратах, стоящих на всех больших телеграфных линиях чаще всего применяется двоичный код Бодо, сопоставляющий каждой букве некоторую последовательность из \term{пяти} простейших сигналов (посылок ток и пауз) одинаковой длительности. Так, как при этом все буквы передаются комбинациями сигналов одной и той же длительности (\term{равномерными кодами}), и поэтому в коде Бодо не требуется спецзнака, отделяющего одну букву от другой - и без того известно, что через каждые 5 элементарных сигналов кончается одна буква и начинается другая. (в приёмных аппаратах такое разделение на комбинации из 5 сигналов производится автоматически) Поскольку, комбинируя две возможности для первого сигнала с двумя возможностями для второго и.т.д. мы можем в результате составить всего $2^5 = 32$ различных комбинации, поэтому код Бодо в его простейшей форме позволяет передавать 32 различных буквы.
	
	В некоторых телеграфных аппаратах кроме простого включения и выключения тока можно также изменять его направление на обратное (3 элементарных сигнала) Возможны также ещё более сложные телеграфные аппараты, в которых посылки тока различаются не только по направлению, но и по силе тока, тем самым увеличивая число элементарных сигналов.
	
	Увеличение числа элементарных сигналов позволяет сделать код более сжатым (т.е. уменьшить число сигналов, требующихся для передачи данного сообщения. Однако, вместе с тем, оно усложняет и удорожает систему передачи. Поэтому в технике связи всё-же предпочтительно применяются коды с малым числом элементарных сигналов.
		

	\subsubsection{Экономность кода}
	
	Сформулируем основную математическую задачу, с которой приходится иметь дело в технике связи. Пусть имеется сообщение, записанное при помощи некоторого алфавита, содержащего n символов. Требуется \term{закодировать} это сообщение, т.е. указать правило, сопоставляющее каждому такому сообщению определённую последовательность из m различных элементарных сигналов ($m=2$ - двоичный, $m=3$ - троичный)
	
	Как выгоднее всего это сделать?  Прежде всего надо объяснить, в каком смысле понимается слово выгоднее. Будем считать кодирование тем более выгодным, чем меньше элементарных сигналов требуется затратить на передачу сообщения. 
	
	Если считать, что каждый из элементарных сигналов продолжается одно и то же время, то наиболее выгодный код позволит затратить на передачу сообщение меньше всего времени. Поэтому переход к более выгодному коду, позволяющему увеличивать эффективность использования данной линии связи имеет несомненное практическое значение. Постараемся подробнее разобраться в том, какие вообще бывают коды. Для определённости будем считать, что код двоичный. В этом случае кодирование состоит в том, что каждой из $m$ букв нашего алфавита сопоставляется какая-то последовательность двух элементарных сигналов - \term{кодовое обозначение} этой буквы. Элементарные сигналы заменим цифрами $0$ и $1$ (например, посылка тока - $1$, отсутствие тока - $0$). Требуется ещё, чтобы закодированное сообщение можно было \term{однозначно декодировать}, т.е, чтобы в длинной последовательности из нулей и единиц, сопоставляемой многобуквенному сообщению всегда можно было понять, где кончается кодовое обозначение одной буквы и начинается другой.
	
	
	
%\wdate{14.11.13}


	
	Нетрудно понять, какой код будет наиболее выгодным: будем измерять \term{выгодность} (экономность) данного двоичного кода при помощи максимального числа элементарных сигналов (цифр) из $0$ и $1$, требующегося для передачи одной буквы. Можно показать, что наибольшее число $k$ элементарных сигналов, приходящихся на $1$ букву удовлетворяет неравенству $k \ge \log n$ ($n$- число букв алфавита). Этот факт объясняется соображениями теории информации: одна буква $N$ - буквенного алфавита может содержать информацию, равную $1\mbox{б} = \log n$, а каждый передаваемый элементарный сигнал, принимающий одно из двух значений (посылка тока или пауза) может содержать информацию не более, чем 1 бит. Поэтому, для передачи одной буквы надо, чтобы число элементарных сигналов было не менее $\log n$.
	



	\subsubsection{Метод двоичной системы счисления}

	

		Этот метод используется для построения наиболее экономного двоичного кода. В десятичной системе счисления каждое число представляется в виде суммы степеней числа $10$: $k = a_k \cdot 10^k +a_{k-1} \cdot 10^{k-1} + \sudots + a_0 \cdot 10^0$, где числа $a_k, \sudots ,a_1,a_0$ - цифры числа, принимающие значения от $0$ до $9$. Число $N$ при этом обозначается последовательностью своих цифр $a_k a_{k-1} \sudots a_0$. Аналогично этому число $n$ можно представить в виде степеней числа $2$: $n = b_l \cdot 10^l + \sudots + b_0 \cdot 10^0$, где числа $b_l, \sudots ,b_0$ - цифры числа, принимающие значения $0$ и $1$. Число $n$ обозначается последовательностью соответствующих цифр ($6 = 1*2^2 + 1*2^1 + 0 * 2^0$). Аналогично можно представить число в виде суммы степеней числа $m$ - т.н. $m$-ичная система счисления. 
		
		Число $k$ цифр в десятичной системе для записи числа $n$ удовлетворяет неравенству $10^{k-1} \le n < 10^k$. В промежутке между $10^1$ и $10^2 - 1$ все цифры будут двузначными, а в промежутке $10^2 .. 10^3 - 1$ - трёхзначными и.т.д.
		В двоичной системе счисления число $k$ удовлетворяет неравенству $2^{k-1} \le n < 2^k$, значит, число $6$ - трёхзначное, а число $9$ - четырёхзначное. При добавлении к двоичной записи ведущих нулей, мы придём к \term{равномерному} двоичному коду для $n$-буквенного алфавита с минимальной длиной кодового обозначения $k$.
		


	\subsubsection{Код Шеннона - Фано}
		
	
	
	Основной результат предыдущего параграфа : если число букв в алфавит равно $n$, а число используемых элементарных сигналов равно $m$, то при любом методе кодирования, среднее число $k$ элементарных сигналов, приходящихся на одну букву алфавита должно быть $k > \frac{\log n}{\log m}$. (из неравенства $2^{k-1} \le n < 2^k \Rightarrow \log n < k \cdot \log m$).
	
	На самом деле, результаты предыдущих параграфов могут быть значительно улучшены, если воспользоваться введённым понятием энтропии и учесть статистические свойства реальных сообщений. Рассмотрим далее простейший случай сообщений, записанных при помощи некоторых $n$ букв нашего алфавита, частоты появления которых на любом месте сообщения характеризуются вероятностями $p_1, \sudots ,p_n$, причём $\sum\limits_n p_n = 1$. Вероятность $p_i$ появления $i$-й буквы на \term{любом} месте сообщения принимается одной и той же вне зависимости от того, какие буквы стояли на всех предыдущих местах. Последовательные буквы сообщения предполагаются \term{независимыми} друг от друга.
	
	Будем далее рассматривать только двоичные коды с элементарными сигналами $0$ и $1$. Среднее число k двоичных элементарных сигналов, приходящихся в закодированном сообщении на $1$ букву исходного сообщения должно быть больше числа энтропии $H$ : ($k > H = -p_1 \log p_1 - p_2 \log p_2 - \sudots - p_n \log p_n$), где $H$ - энтропия опыта, состоящего в распознавании одной буквы текста (энтропии одной буквы). Отсюда сразу следует, что при любом методе кодирования, для записи сообщения из $M$ букв не меньше $MH$ элементарных сигналов.
	
	Если вероятности $p1, \sudots ,p_n$ - не все равны между собой, тогда $H < \log n = H(\alpha_0)$, поэтому учёт статистических закономерностей сообщения может позволить построить код более экономный, чем наилучший равномерный код, требующий $M \cdot \log n$ двоичных знаков для записи текста из $M$ букв.
	
	Код Шеннона - Фано
	
	Для получения наиболее экономного кода удобно начать с того, чтобы расположить все $n$ букв алфавита в один столбец в порядке убывания вероятностей. Затем все эти буквы следует разбить на $2$ группы: \term{верхнюю} и \term{нижнюю}, так, чтобы суммарные вероятности для буквы сообщения принадлежать каждой из этих групп были возможно \term{более близки} одна к другой. Для букв первой группы в качестве первой цифры кодового обозначения используется $1$, $а$ для букв второй группы - $0$.
	
	
%\next		
	
	
	Далее, каждую из $2$ полученных групп снова надо разделить на $2$ части с возможно более близкой суммарной вероятностью. В качестве второй цифры кодового обозначения используют $1$ или $0$ в зависимости, принадлежит ли наша буква к первой или ко второй подгруппе. Процесс повторяется до тех пор, пока мы не придём к группам, каждая из которых не содержит по одной букве. Такой метод кодирования сообщений был впервые предложен в $1948$-$49$ гг. независимо друг от друга Р.Фано, К.Шенноном.
	
	\sample{Пример} Пусть $n=6$ (алфавит из $6$ букв), вероятности которых равны соответственно $0.4; 0.2; 0.2; 0.1; 0.05; 0.05.$
	
	На первом этапе деления на группы отщепим одну первую букву, оставив во второй группе все остальные.
	Далее, вторая буква составит первую подгруппу второй группы	\sudots
	
	\begin{tabular}{|l|c|l|l|}
	\hline
	$N$ буквы & Вероятность & Разбиение на подгруппы & Кодовое обозначение \\
	\hline
	$1$ & $0.4$ & $I$ & $1$ \\
	\hline 
	$2$ & $0.2$ & $II$\quad$I$ & $01$ \\
	\hline 
	$3$ & $0.2$ & $II$\quad$II$\quad$I$ & $001$ \\
	\hline
	$4$ & $0.1$ & $II$\quad$II$\quad$II$\quad$I$ & $0001$ \\
	\hline
	$5$ & $0.05$& $II$\quad$II$\quad$II$\quad$II$\quad$I$ & $00001$ \\
	\hline
	$6$ & $0.05$& $II$\quad$II$\quad$II$\quad$II$\quad$II$ & $00000$ \\
	\hline
	
	\end{tabular}
	
	Выводы:  Основной принцип, положенный в основу кодирования по методу Шеннона - Фано заключается в том, что при выборе каждой цифры кодового обозначения мы стараемся, чтобы содержащееся в ней количество информации было наибольшим, т.е. чтобы независимо от значения всех предыдущих цифр эта цифра принимала оба возможных для неё значений 0 и 1 о возможности с одинаковой вероятностью. Существенно, что буква, имеющая большую вероятность в коде Шеннона - Фано соответствуют более короткие кодовые обозначения (эти буквы быстрее оказываются выделенными в отдельную группу.
	В результате среднее значение k длины такого кодового обозначения оказывается только немногим большим минимального значения H, допускаемого соображениями сохранения количества информации при кодировании.
	Так, для рассмотренного выше примера шестибуквенного алфавита наилучший равномерный код состоит из трёхзначных кодовых обозначений (т.к. $2^2 \le 6 < 2^3$), поэтому в нём на каждую букву приходится ровно 3 элементарных сигнала. При использовании кода Шеннона - Фано среднее число $k$ элементарных сигналов, приходящихся на одну буку равно $M_\xi = \sum L \cdot \mbox{принимаемые значения вероятности} = 1*0.4 + 2*0.2 + 3*0.2 + 4*0.1 +5*(0.05+0.05) = 2.3$. Это значение заметно меньше, чем 3, но не очень сильно отличается от энтропии $H = -0.4 \log 0.4 - 2*0.2 \log 0.2 - 0.1 \log 0.1 - 2*0.05 \log 0.05 \approx 2.22.$



	\subsubsection{Код Хаффмана}



	Близким к коду Шеннона-Фано, но ещё более выгодным является код Хаффмана. Построение этого кода опирается на простое преобразование того алфавита, на котором записываются передаваемые по линиям связи сообщения. Это преобразование  называется \term{сжатием} алфавита. 
	
	Пусть имеется алфавит $A$, содержащий буквы $A_1, A_2, \sudots ,A_n$, вероятности появления которых в сообщении соответственно равны $p_1, p_2, \sudots ,p_n$. При этом мы считаем буквы расположенными в порядке убывания их вероятности ($p_1 \ge p_2 \ge \sudots \ge p_n$). Условимся не различать между собой две \term{наименее вероятные} буквы алфавита $(a_{n-1},a_n)$ - новая буква $b$ алфавита $A_1: a_1, a_2, \sudots ,a_{n-2}, b$ и вероятностью появления $p_1,p_2, \sudots ,p_{n-2},(p_{n-1}+p_n)$. Алфавит $A_1$ называется алфавитом, подученным из алфавита $A$ c помощью одной операции сжатия. Расположим буквы нового алфавита $A_1$ в порядке убывания их вероятностей и подвергнем сжатию алфавит $A_1$. При этом мы придём к алфавиту $A_2$, получаемому из алфавита $A$ двукратным сжатием. Продолжая эту процедуру мы будем приходить ко всё более коротким алфавитам. После $N-2$-кратного сжатия мы придём к алфавиту $A_{n-2}$, содержащему всего $2$ буквы.
	
	\sample{Пример} Пусть $n=6$ (алфавит из $6$ букв), вероятности которых равны соответственно $0.4;0.2;0.2;0.1;0.05;0.05$.
	
	Рассмотрим наш алфавит из 6 букв с соответствующими вероятностями:
	
	\begin{tabular}{|l|c|c|c|c|r|}
	\hline				% Сжатые алфавиты
	$N$ буквы & Исх алфавит A   & $A_1$ & $A_2$ & $A_3$  & $A_4$ \\
	\hline
	$1$ & $0.4$ $0$      & $0.4$ $0$    & $0.4$ $0$   & $0.4$ $0$  & $0.6$ $1$\\
	\hline                                                        %^>
	$2$ & $0.2$ $10$     & $0.2$ $10$   & $0.2$ $10$  & $0.4$ $11$ & $0.4$ $0$ \\
	\hline                             %^>
	$3$ & $0.2$ $111$    & $0.2$ $111$  & $0.2$ $111$ & $0.2$ $10$ &  \\
	\hline                             %^
	$4$ & $0.1$ $1101$   & $0.1$ $1101$ & $0.2$ $110$ &  &  \\
	\hline                             %^
	$5$ & $0.05$ $11001$ & $0.1$ $1100$ &  &  &  \\
	\hline              %^
	$6$ & $0.05$ $11000$ &  &  &  &  \\
	\hline
	\end{tabular}
	
	Условимся приписывать двум буквам последнего алфавита кодовые обозначения $1$ и $0$. Если кодовые обозначения уже приписаны всем буквам алфавита $A_j$, то буквам предыдущего алфавита $A_{j-1}$, сохранившимся и в алфавите $A_j$ мы припишем те-же кодовые обозначения, которые они имели в алфавите $A_j$. Двум буквам алфавита $A_{j-1}: A',A"$, слившимся в одну букву $b$ алфавита $A_j$ мы припишем обозначения, получающиеся из кодового обозначения буквы b добавлением цифр $1$ и $0$ в конце. 

%\wdate{28.11.2013}
%12 дек. - К/р 1&2 к/р & 1&2тест-я 9^30 - 10^50  2157 2132 2133 + зачёт

Легко увидеть, что из самого построения получаемого таким образом кода Хаффмана вытекает, что он удовлетворяет \term{условию}: никакое кодовое обозначение не является здесь начало другого, более длинного, кодового обозначения. Заметим также, что кодирование некоторого алфавита по методу Хаффмана также, как и по методу Шеннона - Фано не является однозначно определённой процедурой, однако отметим, что среднее число $k$ элем сигналов, приходящихся на одну букву алфавита для всех построенных кодов всегда остаётся одинаковым.
	Можно показать, что код Хаффмана является \term{самым экономным} из всех возможных в том смысле,что ни для какого другого метода кодирования букв некоторого алфавита среднее число $k$ элементарных сигналов, приходящихся на одну букву не может быть меньше того, какое получается при кодировании по методу Хаффмана.
	
	

\subsubsection{Основная теорема о кодировании}

	Достигнутая степень близости среднего числа $k$ двоичных знаков, приходящихся на одну букву сообщения к $H$ может быть ещё сколь угодно увеличена при помощи перехода к кодированию всё более и более длинных блоков. Это вытекает из следующего общего утверждения, которое называется основной теоремой о кодировании.
	
	\task{Теорема:} При кодировании сообщения , разбитого на $N$-буквенные блоки можно, выбрав $N$ достаточно большим добиться того, чтобы среднее число $k$ элементарных двоичных сигналов, приходящихся на одну букву исходного сообщения было сколь угодно близко к $H$.
	Замечание: Очень длинное сообщение из $M$ букв может быть закодировано при помощи сколь угодно близкого к числу $MH$ (но большего) числа элементарных сигналов, если только предварительно разбить это сообщение на достаточно длинные блоки из $N$ букв и сопоставлять отдельные кодовые обозначения сразу целым блокам.
	Методы кодирования блоков могут быть самыми различными (например, можно использовать методы Шеннона - Фано, Хаффмана)
	


\subsubsection{m-ичные коды}

	Содержание предыдущих параграфов легко переносится и на случай $m$-ичных кодов, использующих $m$ элементарных сигналов. Так, например для построения $m$-ичных кодов Шеннона - Фано надо лишь разбивать группы символов не на $2$, а на $m$ частей, по возможности близкой суммарной вероятности, а для построения $m$-ичного кода Хаффмана надо использовать операцию сжатия алфавита, при которой каждый раз сливаются не две, а $m$ букв исходного алфавита, имеющих наименьшие вероятности.

	Ввиду важности кодов Хаффмана остановимся на этом вопросе подробнее. Сжатие алфавита, при котором $m$ букв заменяются на одну приводит к уменьшению числа букв на $m-1$. Так, как для построения $m$-ичного кода, очевидно, требуются, чтобы последовательность сжатий привела нас к алфавиту из $m$ букв (сопоставляемых $m$ сигналам кода), то необходимо, чтобы число $n$ букв первоначального алфавита было представимо в виде $n = m + s(m-1)$, где $s$ - целое число сжатий.

	Этого всегда можно добиться, добавив, если нужно, к первоначальному алфавиту ещё несколько "фиктивных букв", вероятности появления которых считаются равными $0$. После этого, построение $m$-ичного кода Хаффмана производится точно так-же, как и в случае двоичного кода.
	
	\sample{Пример:} В случае алфавита из $6$ букв, имеющих вероятности $0.4; 0.2; 0.2; 0.1; 0.05; 0.05$. Для построения \term{троичного} кода Хаффмана надо присоединить к нашему алфавиту ещё одну фиктивную букву с нулевой вероятностью и поступать, как указано в таблице:

	\begin{tabular}{|l|c|c|c|}
	\hline
	Номер буквы & \multicolumn{3}{c|}{Вероятности и кодовые обозначения} \\
	\hline
	 & Исходный алфавит $A$ & Сжатый алфавит $A_1$ & Сжатый алфавит $A_2$ \\
	\hline
	$1$ & $0.4$ - $0$  &  $0.4$ - $0$  & $0.4$ - $0$ \\
	\hline %                  \
	$2$ & $0.2$ - $2$  &  $0.2$ - $2$ &  $0.4$ - $1$ \\
	\hline %                  |
	$3$ & $0.2$ - $10$ & $0.2$ - $10$ & $0.2$ - $2$  \\
	\hline 	
	$4$ & $0.1$ - $11$ & $0.2$ - $11$ & \\
	\hline %      \          /
	$5$ & $0.05$ - $120$ && \\
	\hline %      |
	$6$ & $0.05$ - $121$ && \\
	\hline %      |
	$7$ & $0$ - ---  && \\
	\hline %      /
	\end{tabular}

	\task{Теорема:} Любые $n$ чисел $k_1, k_2,\sudots, k_n$, удовлетворяющие неравенству $\frac{1}{m^{k_1}} + \frac{1}{m^{k_2}} + \sudots + \frac{1}{m^{k_n}} \le 1 \quad (*).\quad$
	Любые из $k_n$ чисел являются длинами сообщений некоторого $m$-ичного кода, сопоставляющего $n$ буквам алфавита $n$ последовательностей элементарных сигналов, принимающих $m$ возможных значений.
	
	Это утверждение $(*)$ впервые было доказано в $1949$ году американским учёным Л.Крафтом и позднее было обобщено Б. Макмилланом, поэтому неравенство $(*)$ часто называют неравенством Крафта - Макмиллана. Используя неравенство $(*)$ можно получить следующий результат:
	
	\task{Теорема:} основная теорема о кодировании для $m$-ичных кодов. При любом методе кодирования, использующем $m$-мчнный код среднее число $k$ элементарных сигналов, приходящихся на одну букву сообщения никогда не может быть меньше отношения $\frac{H}{\log m}$, где $H$ -  энтропия одной букв сообщения. Однако, оно всегда может быть сделано сколь угодно близким к этой величине, если кодировать сразу достаточно длинные блоки из $N$ букв.
	
	\task{Следствие:} Если по линии связи  за единицу времени  можно передать $L$ элементарных сигналов, принимающих $m$ различных значений, то скорость \term{передачи сообщений} по такой линии не может быть большей, чем $v = \frac{L \cdot \log m}{H} \left[ \frac{\mbox{\scriptsize букв}}{\mbox{\scriptsize ед.времени}} \right]$. Однако, передача со скоростью, сколь угодно близкой к $v$ (но меньшей $v$) является возможной. Величина $C = L \cdot \log m$ зависит лишь от характеристик самой линии связи, в то время, как знаменатель $H$ характеризует передаваемое сообщение. Величина $C$ указывает наибольшее количество единиц информации, которое можно передать по линии связи за единицу времени. Она называется пропускной способностью линии связи.

%\next

	\subsubsection{Энтропия и информация конкретных типов сообщений. Письменная речь.}
	
	Основной результат, полученный ранее состоял в том, что для передачи $M$-буквенного сообщения по линии связи, допускающей $m$ различных элементарных сигналов требуется затратить не меньше, чем $\frac{M \cdot \log n}{\log m}$, где $n$ - число букв алфавита, с помощью которого записано исходное сообщение. При этом существуют методы кодирования, позволяющие сколь угодно близко подойти к границе этой величины.

	Так, как русский телеграфный алфавит содержит $32$ буквы (е=ё, ь=ъ, пробел), то согласно этому результату на передачу $M$-буквенного сообщения  надо затратить $\frac{M \cdot \log 32}{\log m} = \frac{M H_0}{\log m}$, где $H_0 = \log 32 = 5$ бит - энтропия опыта, состоящего в приёме одной буквы русского текста (информация, содержащаяся в одной букве), при условии, что все буквы считаются \term {одинаково вероятными}.
	
	На самом деле, появление в сообщении разных букв совсем не одинаково вероятны. Буквы О, Е встречаются много чаще, чем буквы Ф или Щ. Для более точного вычисления информации, содержащейся в одной букве русского текста надо знать вероятности (частоты) появления различных букв. Ориентировочные значения частот отдельных букв русского языка задаются следующей таблицей:
	
	\begin{tabular}{|l|c|c|c|c|c|c|c|c|}
	\hline	
	Буква\strut & Пробел  & О       & Е,Ё     &  А      & И       & Т       &  Н      & C \\
	Вероятность & $0.175$ & $0.09$  & $0.072$ & $0.062$ & $0.062$ & $0.053$ & $0.053$ &  $0.045$ \\
	\hline	
	Буква\strut & Р       & В       & Л       &  К      & М       & Д       & П       & У \\
	Вероятность & $0.04$  & $0.038$ & $0.035$ & $0.028$ & $0.026$ & $0.025$ & $0.023$ & $0.021$ \\
	\hline	
	Буква\strut & Я       & Ы       & З       & Ь,Ъ     & Б       & Г       &  Ч      &  Й \\
	Вероятность & $0.018$ & $0.016$ & $0.016$ & $0.014$ & $0.014$ & $0.013$ & $0.012$ &  $0.001$ \\
	\hline	
	Буква\strut & Х       & Ж       & Ю       & Ш       & Ц       & Щ       &  Э      &  Ф \\
	Вероятность & $0.009$ & $0.007$ & $0.006$ & $0.006$ & $0.004$ & $0.003$ & $0.003$ &  $0.002$ \\
	\hline
	\end{tabular}
	
	Приравняв эти частоты вероятностям появления соответствующих букв получим для энтропии одной буквы русского текста следующее значение: $H_1 = H(\alpha_1) = -0.175 \cdot \log 0.175 - 0.09 \cdot \log 0.09 - \sudots - 0.002 \cdot \log 0.002 \approx 4.35 \mbox {бит}$.
	
	Из сравнения этого значения с величиной $H_0 = 5$ бит видно, что неравномерность появления различных букв алфавита приводит к уменьшению информации, содержащейся в одной букве русского текста на $\approx 0.65$ бита.
	
	Воспользовавшись этим обстоятельством можно уменьшить число элементарных сигналов, необходимых для передачи $M$-буквенного сообщения до значения $\frac{M \cdot H_1}{\log m}$, т.е. в случае двоичного кода до значения $M \cdot H_1$. Сокращение числа требующихся элементарных сигналов может быть достигнута, например, кодированием отдельных букв русского алфавита по методу Шеннона - Фано.
	


	\subsubsection{Передача непрерывно изменяющихся сообщений. Телевизионные сообщения}
	
	В устной речи, музыке, возможными сообщениями являются уже не последовательности символов или букв, а совокупности звуковых колебаний, которые меняются \term{непрерывным} образом. Мы вполне можем объединить большое число схожих между собой звуков (фонем), если только замена одного из них другим не изменяет смысла прозвучавшего (сказанного). На самом деле, различимо лишь конечное число градаций громкости и высоты тона. Отождествив все звуки, громкость и высоту тона, которые находятся в пределах одной градации, мы снова придём к привычному для нас случаю последовательности сигналов, которые могут принимать только \term{конечное} число различных значений.
	
	Опыт $\beta$ вполне можно заменить новым опытом $\beta \rightarrow \beta_\epsilon$ , получающимся из $\beta$ при помощи отождествления всех его исходов, отличающихся друг от друга меньше, чем на некоторое малое число $\epsilon$.
	
	Энтропию $H_\epsilon$ нового опыта $\beta_\epsilon$ называют $\epsilon$ - энтропией опыта $\beta$. При передаче непрерывно меняющихся сообщений совокупность возможных значений сигнал всегда разбивается на конечное число градаций (ячеек), и все значения в пределах одной градации отождествляются между собой. Эта операция замены непрерывного сообщения новым сообщением, принимающим конечное число возможных значений называется в технике связи \term{квантованием} сообщения.
	
	Важным классом таких непрерывно меняющихся сообщений являются изображения, передаваемые по телевидению или фототелеграфным линиям связи. Любое изображение можно передавать по точкам, каждая из которых является сигналом, принимающим конечное число значений. Учитывается градация яркости. На самом деле, изображения являются неподвижными, на экране ежесекундно сменяется $25$ кадров, создавая впечатление движения.
	
	Общее число элементов(точек), на которые следует разлагать изображение определяется в первую очередь т.н. "разрешающей способностью" глаза, т.е. его способностью различать близкие фрагменты изображения. В современном телевидении это число имеет значение от $200$ - $300$ тыс до $500$ тыс. - $1$ млн. Нетрудно понять, что по этой причине энтропия телевизионного изображения имеет огромную величину.
	



	\pagebreak

	\section{Вопросы к зачёту по курсу ТИ.}
	
	
	\begin{enumerate}
	
	\item	\begin{enumerate}
		\item	Вероятность. Случайные события и случайные величины.
		\item	Свойства вероятности. Сложение и умножение вероятностей. Несовместные и независимые случайные события
		\item	Условные вероятности и их свойства.
		\item	Математическое ожидание случайной величины. Свойства.
		\item	Дисперсия случайной величины. Свойства.
		\end{enumerate}		
	
	\item	\begin{enumerate}
		\item	Энтропия как мера степени неопределённости.
		\item	Энтропия сложных событий. Условная энтропия.
		\item	Свойства условной энтропии
		\item	Задача о болезненной реакции.
		\item	Понятие об информации. Количество информации.
		\item	Задача о шарах и предварительной информации.
		\item	$\epsilon$ -энтропия опыта. Иллюстрация игры в испорченный телефон.
		\item	Условная информация. Свойства условной информации.
		\item	Определение энтропии перечислением её свойств.
		\end{enumerate}		

	\item	\begin{enumerate}
		\item	Кодирование. Основные понятия. Определение кода. Двоичный код. Код Морзе.
		\item	Код Бодо. Равномерный код. Экономность кода
		\item	Метод двоичной системы счисления. 
		\item	Код Шеннона - Фано
		\item	Пример на использование кода Шеннона - Фано
		\item	Код Хаффмана. Сжатие кода. Пример на использование кода Хаффмана
		\item	Задача о кодировании букв русского алфавита по методу Шеннона - Фано
		\end{enumerate}		
	\end{enumerate}		
		
	
	
	\pagebreak

	\section{Литература по кодированию и декодированию}
	
	
	\begin{itemize}
	
	\item	Питерсон У. Коды, исправляющие ошибки. М.:Мир, 1964.
	\item	Добрушин Р.Л. Теория оптимального кодирования информации (сборник "Кибернетика на службе коммунизма" под ред. ак. Берга А.И. Т.3, с. 13-46., 1966г.)
	\item	Элайс П. Кодирование и декодирование. ("Лекции по теории связи" под ред. Е.Багдади.:М, Мир, 1964, с 289 - 317.)
	
	\end{itemize}

\end{document}